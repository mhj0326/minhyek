<!DOCTYPE html>
<html lang="en">
    <head>
        <!-- Document Title -->      
        <title>Minhyek Jeon | Minimal Portfolio HTML Template</title>

        <!-- Metas -->      
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="CaliberThemes" />

        <!-- Favicon -->      
        <link rel="icon" type="image/png" href="assets\I made it\logo_mj.png" />

        <!-- Links -->      
        <link href="https://fonts.googleapis.com/css?family=Roboto:400,500,700&subset=latin,latin-ext" rel="stylesheet" type="text/css" />
        <link rel='stylesheet' id='bootstrap-css'  href='assets/lib/bootstrap/css/bootstrap.min.css' type='text/css' media='all' />
        <link rel='stylesheet' id='font-awesome-css'  href='assets/css/icons/font-awesome.min.css' type='text/css' media='all' />       
        <link rel='stylesheet' id='swiper-css' href='assets/lib/swiper/css/swiper.min.css' type='text/css' media='all' />
        <link rel='stylesheet' id='cubeportfolio-css'  href='assets/lib/cubeportfolio/css/cubeportfolio.min.css' type='text/css' media='all' /> 

        <link rel='stylesheet' id='main-css'  href='style.css' type='text/css' media='all' />

    </head>
    <body class="single single-portfolio portfolio-details-top">

        <!-- Page Wrapper -->
        <div id="page" class="site">
            <header id="masthead" class="site-header standard sticky" role="banner">		
                <div class="container">
                    <div id="site-branding">			
                        <a class="logo-brand" href="index.html">
                            <img class="logo" src="assets\I made it\logo_mj.png" alt="Logo">					                            
                            <img class="retina-logo" src="assets\I made it\logo_mj.png" alt="Retina Logo">					                            
                        </a>	
                    </div><!-- .site-branding -->
                    <span id="ham-trigger-wrap">
                        <span class="ham-trigger">
                            <span></span>                            
                        </span>                            
                    </span>
                    <nav id="site-navigation" class="main-navigation" role="navigation" aria-label="Top Menu">
                        <ul id="top-menu" class="menu">
                            <li class="has-children"><a href="index.html">Home</a>
                                
                            </li>
                            <li class="has-children"><a href="project.html">Project</a>
                                
                            </li>
                            <li class="has-children"><a href="experience.html">Experience</a>
                                
                                
                            </li>
                            <li class="has-children"><a href="publication.html">Publication</a>
                                
                                
                            </li>
                            <li class="has-children"><a href="awards.html">Others</a>
                                
                            </li>
                            
                        </ul>	
                    </nav>
                                             
                </div><!-- .wrap -->

                
	
            </header><!-- #masthead -->

            <div class="site-content-contain">
                <div id="content" class="site-content">                 
                    <div id="primary" class="content-area">
                        <!-- Portfolio Filter -->
                        
                        <main id="main" class="site-main" role="main">        
                            <div class="container t-offset-20">
                                <div class="row">
                                    <div class="col-sm-12">

                                        <article class="portfolio">

                                            <header class="entry-header">
                                                

                                                <h2>Implementation and Comprehensive Testing of the Implicit Transfer Operator Learning Model</h2>
                                                

                                    
                                                

            
                                             

                                                <p>* The research is currently in process</p>
                                                <h4>Introduction</h4>
                                                <h5>- Denoising Diffusion Probabilistic Models</h5>
                                                <img src="assets\I made it\ddpm.png" style="width:900px;" class="portfolio-media-image" alt="Brochure 2" ><br>
                                                

                                             
                                                <p>The diagram illustrats the step-by-step process of data transformation using Denoising Diffusion Probabilistic (DDPM) that we are going to use for ITO model. The model gradually transforms the data distribution p(x<sub>0</sub>) to a simple prior distribution p(x<sub>T</sub>) like a standard Gaussian (noising process). It generates samples from the data distribution p(x<sub>0</sub>) by sampling from p(x<sub>T</sub>) and solving the backward diffusion process (denoising process).
                                                    The starting point (X₀) is the original data, depicted as an image of a human face. 
                                                    The data undergoes a series of transformations represented by Xₜ₋₁, Xₜ, up to Xₜ. Arrows indicate the forward diffusion process, 
                                                    where noise is gradually added to the data, transitioning it towards a simpler prior distribution (p(x_T)), typically a Gaussian distribution. 
                                                    The dotted arrows show the denoising process, where the model learns to reverse the diffusion and generate samples from the data distribution (p(x₀)) by starting from p(x_T) and solving the backward process.</p>
                                            
                                                        
                                                <h5>- Implicit Transfer Operator Learning: Multiple Time-Resolution Surrogates for Molecular Dynamics</h5>
                                                <figure style="display: inline-block; text-align: left;">
                                                    <img src="assets\I made it\itoimg.png" style="width:300px;" class="portfolio-media-image" alt="Brochure 2" >
                                                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    
                                                  </figure>
                                                <img src="assets\I made it\ito2img.png" style="width:400px;" class="portfolio-media-image" alt="Brochure 2" >
                                                <figcaption style="margin-top: 10px;"><p>
                                                    ITO, a framework to learn surrogates of the simulation process with multiple time-resolutions, generates self-consistent stochastic dynamics across multiple time-scales, even when the system is only partially observed.
                                                
                                                    (A) SE3-ITO used for molecular applications. 
                                                    (B) MB-ITO, used for experiments with the Müller-Brown potential. 
                                                    <math xmlns="http://www.w3.org/1998/Math/MathML">
                                                      <mi>&Lambda;</mi><sub>pos</sub>
                                                    </math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
                                                      <mi>&Lambda;</mi><sub>nom</sub>
                                                    </math> are positional and nominal embeddings, respectively. Concat is a concatenation, and MLP is a multi-layer perceptron. Arrows are annotated with input and output shapes.
                                                  </p></figcaption>




                                                
                                                <p>The research focus on testing a framework named ITO for generating surrogate models capable of simulating molecular dynamics across multiple time resolutions. This framework leverages denoising diffusion probabilistic models with an SE(3)-equivariant architecture, enabling the generation of self-consistent stochastic dynamics that are crucial for understanding complex molecular behavior like protein folding. The approach not only demonstrates significant computational efficiency, reducing the need for extensive simulations traditionally required in molecular dynamics, but also maintains accuracy across varying time scales. The model can be represented in following formula:</p>
                                                    <p>
                                                        <math xmlns="http://www.w3.org/1998/Math/MathML">
                                                          <mrow>
                                                            <mi>p</mi>
                                                            <mo>(</mo>
                                                            <msub>
                                                              <mi>x</mi>
                                                              <mrow>
                                                                <mi>t</mi>
                                                                <mo>+</mo>
                                                                <msub>
                                                                  <mi>N</mi>
                                                                  <mi>T</mi>
                                                                </msub>
                                                              </mrow>
                                                            </msub>
                                                            <mo>|</mo>
                                                            <msub>
                                                              <mi>x</mi>
                                                              <mi>t</mi>
                                                            </msub>
                                                            <mo>,</mo>
                                                            <mi>N</mi>
                                                            <mo>)</mo>
                                                            <mo>&#8776;</mo>
                                                            <msup>
                                                              <mi>p</mi>
                                                              <mn>0</mn>
                                                            </msup>
                                                            <mo>(</mo>
                                                            <msub>
                                                              <mi>x</mi>
                                                              <mrow>
                                                                <mi>t</mi>
                                                                <mo>+</mo>
                                                                <msub>
                                                                  <mi>N</mi>
                                                                  <mi>T</mi>
                                                                </msub>
                                                              </mrow>
                                                            </msub>
                                                            <mo>|</mo>
                                                            <msub>
                                                              <mi>x</mi>
                                                              <mi>t</mi>
                                                            </msub>
                                                            <mo>,</mo>
                                                            <mi>N</mi>
                                                            <mo>)</mo>
                                                            <mo>=</mo>
                                                            <mo>&#8747;</mo>
                                                            <mi>p</mi>
                                                            <mo>(</mo>
                                                            <msub>
                                                              <mi>x</mi>
                                                              <mrow>
                                                                <mi>t</mi>
                                                                <mo>+</mo>
                                                                <mo>0</mo>
                                                                <mo>:</mo>
                                                                <mi>T</mi>
                                                              </mrow>
                                                            </msub>
                                                            <mo>|</mo>
                                                            <msub>
                                                              <mi>x</mi>
                                                              <mi>t</mi>
                                                            </msub>
                                                            <mo>,</mo>
                                                            <mi>N</mi>
                                                            <mo>)</mo>
                                                            <mo>&#8290;</mo>
                                                            <mi>d</mi>
                                                            <msup>
                                                              <mi>x</mi>
                                                              <mrow>
                                                                <mn>1</mn>
                                                                <mo>:</mo>
                                                                <mi>T</mi>
                                                              </mrow>
                                                            </msup>
                                                            </mrow>
                                                        </math>
                                                      </p>
                                                      <p>
                                                        In this formula, the probability density function is approximated by 
                                                        <msup>
                                                          <mi>p</mi>
                                                          <mn>0</mn>
                                                        </msup>
                                                        , which is defined as an integral over the joint density of the trajectory 
                                                        <msup>
                                                          <mi>x</mi>
                                                          <mrow>
                                                            <mn>1</mn>
                                                            <mo>:</mo>
                                                            <mi>T</mi>
                                                          </mrow>
                                                        </msup>
                                                        from time 
                                                        <msub>
                                                          <mi>t</mi>
                                                        </msub>
                                                        to 
                                                        <msub>
                                                          <mi>t</mi>
                                                          <mo>+</mo>
                                                          <msub>
                                                            <mi>N</mi>
                                                            <mi>T</mi>
                                                          </msub>
                                                        </msub>
                                                        given the starting point 
                                                        <msub>
                                                          <mi>x</mi>
                                                          <mi>t</mi>
                                                        </msub>
                                                        and the noise parameter 
                                                        <mi>N</mi>
                                                        . 
                                                        <msub>
                                                          <mi>x</mi>
                                                          <mi>t</mi>
                                                        </msub>
                                                        function is the joint density of the backward diffusion process while 
                                                        <mi>d</mi>
                                                        <msup>
                                                          <mi>x</mi>
                                                          <mrow>
                                                            <mn>1</mn>
                                                            <mo>:</mo>
                                                            <mi>T</mi>
                                                          </mrow>
                                                        </msup>
                                                        represents the latent variable. 
                                                      </p>
                                                      <img src="assets\I made it\itoresult2.png" style="width:600px;" class="portfolio-media-image" alt="Brochure 2" >
                                                      <p>
                                                        Reversible protein folding-unfolding of Chignolin with CG-SE3-ITO. Conditional probability densities (orange contours) starting from unfolded (upper panels) and folded (lower panels) protein states, at increasing time-lag (left to right), shown on top of data distribution. Below: time-traces of 106 microsecond MD simulations and ITO simulations on tICs 1 and 2. The two dashed lines correspond to the folded state value in tIC 1 (lower line) and tIC 2 (higher line). Contour lines are based on 10,000 trajectories, generated with ancestral sampling with the length, Δt and time-step 200 ps. For Δt = 200 ns this corresponds to 1000 ancestral samples. Through the diagram, it is infered that the ITO model is particularly promising for feasibility of molecular simulations.
                                                      </p>
                                                
                                                <h5>- Problem with ITO model</h5>
                                                <h6>a) Crashing Properties of ITO model</h6>
                                                <p>During the extensive training phases, we encountered significant challenges related to model stability; ITO models frequently crashed before reaching optimal training points.During our training process, we faced persistent issues with the stability of our ITO models. Despite our efforts, these models consistently crashed before reaching the desired training objectives. This instability posed a major obstacle to our progress. To address this challenge, we devised a solution using PyTorch Lightning that would detect when the model crashed during training. Upon detection, the callback system would then restore the model to its most recent checkpoint, ensuring that training could resume seamlessly from where it left off. This proactive approach helped us maintain continuity in our training process preventing time delays.</p>
                                                <h6>b) Addressing Difficulties with Indistinguishable Atoms</h6>
                                                <p>Another challenge we encountered during the initial stages of our training process was related to the handling of indistinguishable atoms within our models. These atoms presented difficulties during the training phase, leading to suboptimal performance and instability in our models. To overcome this challenge, we used distinguishable atoms first. By focusing on these atoms, which could be more easily differentiated during training, we were able to improve the overall performance and stability of our models. This adjustment proved to be highly effective, leading to more robust training outcomes and better model reliability.</p>
                                                <br>
                                                <h4>Method</h4>
                                                <h5>- Dataset</h5>
                                                <img src="assets\I made it\ito_cln_025.gif" style="width:200px;" class="portfolio-media-image" alt="Brochure 2" >
                                                <figcaption style="margin-top: 10px;">Alanine Dipeptide Simulation</figcaption><br>
                                                <p>Alanine dipeptide simulations are commonly used in molecular dynamics (MD) studies due to their simplicity and effectiveness as a model system. Alanine dipeptide is a small peptide consisting of only a few amino acids, making it an ideal candidate for studying fundamental aspects of peptide behavior and dynamics without the computational complexity associated with larger proteins.
                                                    We focused using the ITO on alanine dipeptide simulation. We utilized data that is openly accessible through MDshare. The MD simulations were conducted using integration time-steps of 2 femtoseconds, with data recorded every 1 picosecond. Although the simulations included explicit solvation, we focused exclusively on the 22 atoms constituting the solute, disregarding any velocity data. As a result, the dataset provides only a partial observation of the system dynamics.</p>
                                                <h5>- Preventing Overfitting with Regularization</h5>
                                                <p>The reason for crashing ITO model is because of overfitting. As the loss function proceed to near zero, the frequency of crashing increased which support this assumption. Overfitting occurs when a model becomes overly tuned to the training data, to the point where it fails to generalize well to new, unseen data, in our case, new data used for training. This means that the model performance significantly degrades when applied to new data.
                                                    As a consequence, the model's predictive power diminishes, and its reliability decreases. This diminished generalization ability can increase the likelihood of model crashes. Particularly, when an overfitted model is applied to new data for prediction, it may produce unexpected results, leading to unforeseen behavior. \\
                                                    Preventing overfitting and improving generalization performance are crucial for avoiding model crashes. Techniques such as regularization or collecting more diverse data for training can help mitigate overfitting. In this case, we tested adding regularization using weight decay and dropout, to prevent improve generalization. Weight Decay reduces overfitting in neural networks by keeping weight values small, typically through L2 regularization, while dropout prevents overfitting by randomly setting a subset of activations to zero during training, encouraging robustness and feature redundancy.\\ However, when adding the regularization the model failed to converge even after required time steps of training. The output of ramachandran plot showed non overlaping countours between MD simulation and SE3-ITO model. This lead to an idea that output of SE3-ITO model which show a strong consistency with corresponding transition densities computed from molecular dynamics simulations is based on highly overfitting the model on data.</p>
                                                <h5>- Evaluation of Training Epochs on Model Stability and Performance</h5>
                                                <p>Despite challenges with model stability that led to frequent interruptions during training, the training procedure was extended to approximately 2 million steps, in accordance with the minimum epoch requirement outlined by the authors. The loss function value reached below 0.1 after 60,000 epochs, indicating a highly overfitted model.

                                                    To comprehensively assess the impact of training duration on model performance, additional evaluations were conducted at intermediate training milestones—specifically, at 1,000,000 and 1,500,000 epochs. These checkpoints were selected to investigate the developmental progress of the model's predictive capabilities and to identify any significant changes or improvements in performance metrics over time.
                                                    
                                                    For each of these training checkpoints, the model's output was analyzed through Ramachandran plots and marginal distribution graphs. The Ramachandran plots, which display the phi and psi dihedral angles for amino acid residues, are crucial for understanding the conformational space of proteins and assessing the physical plausibility of the simulated structures. Meanwhile, the analysis of marginal distributions provided insights into the statistical properties of the model outputs.</p>
                                                
                                                    <figure style="display: inline-block; text-align: left;">
                                                        <img src="assets\I made it\training.png" style="width:400px;" class="portfolio-media-image" alt="Brochure 2" >
                                                        <figcaption style="margin-top: 10px;">Training progression over epochs (top) and the corresponding decrease in training loss (bottom) across 150,000 global steps} -  The sharp decrease in loss early in training suggests that the model quickly learns the dominant patterns from the data.</figcaption>
                                                      </figure>
                                                      <figure style="display: inline-block; text-align: left;">
                                                        <img src="assets\I made it\result1.png" style="width:400px;" class="portfolio-media-image" alt="Brochure 2" >
                                                        <figcaption style="margin-top: 10px;">ITO model simluation after 1,000,000 epochs (top) and 1,500,000 epochs} -  Even though the model is overfitting to the data, ITO model shows difference in predicting along MD simulation.</figcaption>
                                                      </figure>
                                                <h5>- Saving the Model Based on most recent checkpoint</h5>
                                                <p>
                                                    The training process, as depicted in the loss graph, exhibits a sharp decrease in loss, which gradually stabilizes near zero after approximately 100,000 steps. After 100,000, we can infer that model is overfitting. However, for the specific objectives of our research, where model stability over long simulations is critical, allowing the model to overfit might be necessary. In our experimental setup, we opted to save the most recent model instead of the one with the smallest loss to better capture the molecular dynamics we are studying. 
                                                  </p>
                                                <br>
                                                <h4>Result</h4>
                                                <img src="assets\I made it\ito_result.png" style="width:500px;" class="portfolio-media-image" alt="Brochure 2" >
                                                <figcaption style="margin-top: 10px;">Expected result from ITO model: There is Strong agreement between the ancestrally and directly sampled transition densities 
                                                    Also there is strong consistency with corresponding transition densities computed from molecular dynamics simulations. 
                                                </figcaption> <br>

                                                <h5>- Different Plot for Different epochs of Model Training</h5>
                                                <p>Despite deliberately overfitting the model through extensive training, the Implicit Transfer Operator (ITO) model exhibits significant deviations from the Molecular Dynamics (MD) simulations in terms of prediction accuracy. The ITO model was subjected to an aggressive training, accumulating up to 1,500,000 epochs. This level of training was intended to closely adapt the model to the training dataset through overfitting. Overfitting typically enables a model to replicate the training data with high precision but at the cost of generalizability to new, unseen data. Despite the model’s tailored fit to the training data, comparisons between the ITO model's predictions and the results from MD simulations reveal clear distinctions. The graphs on figure 3 illustrate this by showing the frequency distributions of the dihedral angles Phi and Psi, which are crucial for understanding the conformational states of molecules in simulations.
                                                    Furthermore, despite increasing the training from 1,000,000 to 1,500,000 epochs, there were no significant improvements observed in the model's performance. The comparisons between the two training checkpoints indicate that extending the training duration beyond 1,000,000 epochs did not lead to appreciable differences in the prediction accuracy or the model's ability to align with Molecular Dynamics (MD) simulation results.</p>
                                                    <figure style="display: inline-block; text-align: left;">
                                                        <img src="assets\I made it\result_ito.png" style="width:700px;" class="portfolio-media-image" alt="Brochure 2" >
                                                        <figcaption style="margin-top: 10px;">ITO model simluation after 1,000,000 epochs (top) and 1,500,000 epochs} -  Even though the model is overfitting to the data, ITO model shows difference in predicting along MD simulation.</figcaption>
                                                      </figure>
                                                <p>The Ramachandran plots provided for the Implicit Transfer Operator (ITO) model across two distinct training checkpoints further underscore the lack of significant differences in model performance despite an increase in training epochs. These plots, which graphically represent the distribution of phi and psi angles of amino acids, show a similar spread and density of points at both stages of the model’s training.</p>
                                                
                                                <br>
                                                <h4>Discussion</h4>
                                                <p>In the course of training, it was observed that approximately 10 hours into the training session, the duration required to complete one epoch increased, correlating with a decline in GPU performance, as depicted in the figure 5. Despite thorough examination of the log files, no explicit issues were identified. However, it is hypothesized that the extended epoch duration may be attributed to several potential factors.</p>
                                                <figure style="display: inline-block; text-align: left;">
                                                    <img src="assets\I made it\gpu.png" style="width:700px;" class="portfolio-media-image" alt="Brochure 2" >
                                                    <figcaption style="margin-top: 10px;">ITO model simluation after 1,000,000 epochs (top) and 1,500,000 epochs} -  Even though the model is overfitting to the data, ITO model shows difference in predicting along MD simulation.</figcaption>
                                                  </figure>
                                                <p>Initially, it was posited that the issue might stem from memory swapping. In instances where GPU or system memory is inadequate for the tasks at hand, the system may resort to swapping data to disk. This process is slower than accessing data stored in RAM or GPU memory, leading to significant increases in epoch duration. To address this, modifications were made to the checkpoint strategy; instead of saving every checkpoint between 10 epochs, the model was adjusted to retain only the 10 most recent checkpoints. Despite these adjustments, the model continued to exhibit performance degradation after 10 hours. This persistent issue is likely related to the architectural specifics of the ITO model, although the exact cause remains elusive.</p>
                                                <br>
                                                <h4>Conclusion</h4>
                                                <p>The overall attempt to replicate the results initially demonstrated in the original paper was unsuccessful, leaving considerable doubt regarding the efficacy of the modeling approach used. Specifically, the strategy of deliberately overfitting the model to a particular dataset raised questions about the appropriateness and reliability of its predictive capabilities.

                                                    Despite intensive training aimed at closely aligning the model with the training data characteristics, the results failed to reproduce the expected outcomes or match the performance benchmarks set by previous studies. This outcome suggests that overfitting, although ensuring high accuracy on training data, does not necessarily translate into effective prediction on unseen or test data. It implies a critical limitation in the model’s ability to generalize, which is essential for robust predictive models in practical applications.
                                                    
                                                    Furthermore, the lack of improvement in the model’s performance, even after significantly increasing the number of training epochs, indicates a plateau in learning that further exacerbates concerns regarding the efficiency of continuing such a training approach. It underscores the possibility that merely fitting the model more closely to the training data might not address underlying issues of model architecture or data representativeness.</p>
                                                
                                                

                                                


   
                                                    
                                                  


                                                
                                                

                      




                                                
                                                
                                                

                        
                                                    
                    
                                                <div class="entry-meta">

                                                    <div class="meta-desc">
                                                        <p>
                                                            
    

</div>






                                                  


                           


                                

 
        <!-- Page Wrapper Ends -->

        <!--  Scripts -->
        <script type='text/javascript' src='assets/js/vendor/jquery-1.12.4.min.js'></script>
        <script type='text/javascript' src='assets/js/vendor/TweenMax.min.js'></script>        
        <script type='text/javascript' src='assets/js/vendor/headsup.min.js'></script>        
        <script type='text/javascript' src='assets/js/vendor/jquery.easing.min.1.3.js'></script>      
        <script type='text/javascript' src='assets/lib/cubeportfolio/js/jquery.cubeportfolio.min.js'></script>
        <script type='text/javascript' src='assets/lib/swiper/js/swiper.min.js'></script>        

        <script type='text/javascript' src='assets/js/main.js'></script>       
        <!--  Scripts Ends -->
    </body>
</html>

                        </main><!-- #main -->
                    </div><!-- #primary -->  
                </div><!-- .site-content-contain -->  
            </div><!-- .site-content-contain -->  
            <footer id="footer" class="site-footer standard" role="contentinfo">
                <div class="container">
                    <div class="site-info">			
                        <p class="copyright">
                            © 2024 Minhyek Jeon	
                        </p>
                    </div>    
                    <nav class="footer-socials" role="navigation" aria-label="Footer Social Links Menu">                           
                        <ul id="social-media-footer" class="social-links-menu">
                            
                            <li><a href="https://www.linkedin.com/in/minhyekjeon"><i class="fab fa-linkedin"></i></a></li>
                            <li><a href="https://github.com/mhj0326"><i class="fab fa-github"></i></a></li>                               
                        </ul>
                    </nav>                        
                </div>            
            </footer>
        </div><!-- #page -->
    </body>
</html>
